{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text_Detection_From_Image_Python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3aNHbDQH5ekk",
        "outputId": "eeaa95d2-36aa-4f34-bb13-38a03fb8704f"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UncuC7w_3Hh",
        "outputId": "a77b9e15-8716-4a47-f470-fe3070c7ec60"
      },
      "source": [
        "!git clone https://github.com/clovaai/CRAFT-pytorch.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CRAFT-pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-5O3nTtSXXJ",
        "outputId": "b5349998-b502-43de-b091-416e74713221"
      },
      "source": [
        "#!git clone https://github.com/sujaykhandekar/Automated-objects-removal-inpainter.git\n",
        "!git clone https://github.com/clovaai/deep-text-recognition-benchmark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deep-text-recognition-benchmark' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCEastO3un2e",
        "outputId": "7e027143-2e67-4b6e-d87e-c3ac0f3760ee"
      },
      "source": [
        "!pip install ipyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipyplot in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from ipyplot) (5.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from ipyplot) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ipyplot) (1.21.6)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.7/dist-packages (from ipyplot) (1.0.9)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->ipyplot) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->ipyplot) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->ipyplot) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->ipyplot) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M92AvpR5prs"
      },
      "source": [
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds37pAT958b_"
      },
      "source": [
        "sys.path.append('/content/CRAFT-pytorch')\n",
        "sys.path.append('/content/deep-text-recognition-benchmark')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJCesIAzEk6F"
      },
      "source": [
        "# create dir for models\n",
        "% mkdir weights\n",
        "# craft main\n",
        "!wget -O weights/craft_mlt_25k.pth https://drive.google.com/uc?id=1Jk4eGD7crsqCCg9C9VjCLkMN3ze8kutZ&export=download\n",
        "# craft refiner\n",
        "!wget -O weights/craft_refiner_CTW1500.pth https://drive.google.com/uc?id=1XSaFwBkOaFOdtk4Ane3DFyJGPRw6v5bO&export=download\n",
        "# deep text recongnition models\n",
        "!wget -O weights/TPS-ResNet-BiLSTM-Attn.pth https://www.dropbox.com/sh/j3xmli4di1zuv3s/AADbTu4LF-nMUBmC43_RQ8OGa/TPS-ResNet-BiLSTM-Attn.pth?dl=1\n",
        "!wget -O weights/TPS-ResNet-BiLSTM-CTC.pth https://www.dropbox.com/sh/j3xmli4di1zuv3s/AAB0X-sX05-0psb4uXWPYSmza/TPS-ResNet-BiLSTM-CTC.pth?dl=1\n",
        "!gdown --id 1ajONZOgiG9pEYsQ-eBmgkVbMDuHgPCaY -O weights/TPS-ResNet-BiLSTM-Attn-case-sensitive.pth "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hju9eTxO7MkX"
      },
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from shapely.geometry import Point, Polygon\n",
        "\n",
        "import ipyplot\n",
        "\n",
        "#### Craft imports\n",
        "# import sys\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import craft_utils\n",
        "import imgproc\n",
        "import file_utils\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "from craft import CRAFT\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "####### Craft Dataset\n",
        "\n",
        "import re\n",
        "import six\n",
        "import math\n",
        "import lmdb\n",
        "import torch\n",
        "\n",
        "from natsort import natsorted\n",
        "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
        "from torch._utils import _accumulate\n",
        "import torchvision.transforms as transforms\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHsC8oxCkIJP"
      },
      "source": [
        "#### Craft imports\n",
        "\n",
        "\n",
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in (\"yes\", \"y\", \"true\", \"t\", \"1\")\n",
        "\n",
        "\n",
        "def create_craft_args(refine):\n",
        "\n",
        "# --trained_model: pretrained model\n",
        "# --text_threshold: text confidence threshold\n",
        "# --low_text: text low-bound score\n",
        "# --link_threshold: link confidence threshold\n",
        "# --cuda: use cuda for inference (default:True)\n",
        "# --canvas_size: max image size for inference\n",
        "# --mag_ratio: image magnification ratio\n",
        "# --poly: enable polygon type result\n",
        "# --show_time: show processing time\n",
        "# --test_folder: folder path to input images\n",
        "# --refine: use link refiner for sentense-level dataset\n",
        "# --refiner_model: pretrained refiner model\n",
        "\n",
        "    args = argparse.Namespace()\n",
        "    args.trained_model = 'weights/craft_mlt_25k.pth'\n",
        "    # args.text_threshold = 0.6 # 0.7\n",
        "    # args.low_text = 0.35 # 0.4\n",
        "    # args.link_threshold = 0.6 # 0.4\n",
        "    args.text_threshold = 0.6 # 0.7\n",
        "    args.low_text = 0.35 # 0.4\n",
        "    args.link_threshold = 0.7 # 0.4\n",
        "    if torch.cuda.is_available():\n",
        "        args.cuda = True\n",
        "    else:\n",
        "        args.cuda = False\n",
        "    args.canvas_size = 1280\n",
        "    args.mag_ratio = 1.5\n",
        "    args.poly = False\n",
        "    args.show_time = False\n",
        "    args.test_folder = './data'\n",
        "    args.refine = refine\n",
        "    args.refiner_model = 'weights/craft_refiner_CTW1500.pth'\n",
        "    return args\n",
        "\n",
        "\"\"\" For test images in a folder \"\"\"\n",
        "# image_list, _, _ = file_utils.get_files(args.test_folder)\n",
        "\n",
        "# result_folder = './result/'\n",
        "# if not os.path.isdir(result_folder):\n",
        "#     os.mkdir(result_folder)\n",
        "\n",
        "def test_net(args, net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # resize\n",
        "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n",
        "    ratio_h = ratio_w = 1 / target_ratio\n",
        "\n",
        "    # preprocessing\n",
        "    x = imgproc.normalizeMeanVariance(img_resized)\n",
        "    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
        "    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
        "    if cuda:\n",
        "        x = x.cuda()\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        y, feature = net(x)\n",
        "\n",
        "    # make score and link map\n",
        "    score_text = y[0,:,:,0].cpu().data.numpy()\n",
        "    score_link = y[0,:,:,1].cpu().data.numpy()\n",
        "\n",
        "    # refine link\n",
        "    if refine_net is not None:\n",
        "        with torch.no_grad():\n",
        "            y_refiner = refine_net(y, feature)\n",
        "        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n",
        "\n",
        "    t0 = time.time() - t0\n",
        "    t1 = time.time()\n",
        "\n",
        "    # Post-processing\n",
        "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n",
        "\n",
        "    # coordinate adjustment\n",
        "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
        "    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
        "    for k in range(len(polys)):\n",
        "        if polys[k] is None: polys[k] = boxes[k]\n",
        "\n",
        "    t1 = time.time() - t1\n",
        "\n",
        "    # render results (optional)\n",
        "    render_img = score_text.copy()\n",
        "    render_img = np.hstack((render_img, score_link))\n",
        "    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n",
        "\n",
        "    if args.show_time : print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n",
        "\n",
        "    return boxes, polys, ret_score_text\n",
        "\n",
        "\n",
        "def create_text_mask(args, image_array, debug=False):\n",
        "    # load net\n",
        "    net = CRAFT()     # initialize\n",
        "\n",
        "    if debug:\n",
        "        print('Loading weights from checkpoint (' + args.trained_model + ')')\n",
        "\n",
        "    if args.cuda:\n",
        "        net.load_state_dict(copyStateDict(torch.load(args.trained_model)))\n",
        "    else:\n",
        "        net.load_state_dict(copyStateDict(torch.load(args.trained_model, map_location='cpu')))\n",
        "\n",
        "    if args.cuda:\n",
        "        net = net.cuda()\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = False\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # LinkRefiner\n",
        "    refine_net = None\n",
        "    if args.refine:\n",
        "        from refinenet import RefineNet\n",
        "        refine_net = RefineNet()\n",
        "        \n",
        "        if debug:\n",
        "            print('Loading weights of refiner from checkpoint (' + args.refiner_model + ')')\n",
        "\n",
        "        if args.cuda:\n",
        "            refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model)))\n",
        "            refine_net = refine_net.cuda()\n",
        "            refine_net = torch.nn.DataParallel(refine_net)\n",
        "        else:\n",
        "            refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model, map_location='cpu')))\n",
        "\n",
        "        refine_net.eval()\n",
        "        args.poly = True\n",
        "\n",
        "    t = time.time()\n",
        "\n",
        "    image = image_array\n",
        "\n",
        "    bboxes, polys, score_text = test_net(args, net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly, refine_net)\n",
        "\n",
        "    final_bboxes = np.around(bboxes).astype(int)\n",
        "\n",
        "    final_polys = [np.around(x).astype(int).reshape(-1).tolist() for x in polys]\n",
        "    # final_polys = [np.around(x).astype(int) for x in polys]\n",
        "\n",
        "    # return final_bboxes, final_polys\n",
        "    return bboxes, polys, score_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BOW4x7PYGOb"
      },
      "source": [
        "def display_image_boxes(image_array, boxes):\n",
        "    import PIL.ImageDraw as ImageDraw\n",
        "    import PIL.Image as Image\n",
        "    mask = Image.new(\"L\", (image_array.shape[1], image_array.shape[0]))\n",
        "    draw = ImageDraw.Draw(mask)\n",
        "    for i in range(len(boxes)):\n",
        "        draw.polygon(boxes[i], outline=256, fill=256)\n",
        "    display(mask)\n",
        "\n",
        "\n",
        "def transform_bboxes_to_rectangles(bboxes):\n",
        "    rectangles = [[[x.min(axis=0)[0], x.min(axis=0)[1]],\n",
        "                    [x.max(axis=0)[0], x.min(axis=0)[1]],\n",
        "                    [x.max(axis=0)[0], x.max(axis=0)[1]],\n",
        "                    [x.min(axis=0)[0], x.max(axis=0)[1]]] for x in bboxes]\n",
        "    return np.array(rectangles)\n",
        "\n",
        "def create_cutted_images_list(image_array, rectangles):\n",
        "    list_ = []\n",
        "    print(rectangles)\n",
        "    for i in range(len(rectangles)):\n",
        "        # display(Image.fromarray(np.array(mask)))\n",
        "        x = rectangles[i].astype(int)\n",
        "        list_.append(image_array[\n",
        "        x[1][1]:x[3][1], \n",
        "        x[0][0]:x[2][0], \n",
        "        :\n",
        "        ])\n",
        "        \n",
        "    return list_\n",
        "\n",
        "def get_image_mask_from_boxes(image_array, boxes):\n",
        "    import PIL.ImageDraw as ImageDraw\n",
        "    import PIL.Image as Image\n",
        "    mask = Image.new(\"L\", (image_array.shape[1], image_array.shape[0]))\n",
        "    draw = ImageDraw.Draw(mask)\n",
        "    for i in range(len(boxes)):\n",
        "        draw.polygon(boxes[i], outline=256, fill=256)\n",
        "    return np.array(mask)\n",
        "\n",
        "def create_word_2_sentence_index(word_bboxes, sentence_bboxes):\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for w_idx in range(len(word_bboxes)):\n",
        "        arr = np.array([])\n",
        "        for s_idx in range(len(sentence_bboxes)):\n",
        "            word_polygon = Polygon(word_bboxes[w_idx])\n",
        "            sent_polygon = Polygon(sentence_bboxes[s_idx])\n",
        "            share_intersection = word_polygon.intersection(sent_polygon).area/word_polygon.area\n",
        "            arr = np.append(arr, share_intersection)\n",
        "        \n",
        "        result[w_idx] = arr.argmax()\n",
        "        # print(arr.tolist())\n",
        "    \n",
        "    return list(result.items())\n",
        "\n",
        "def create_word_2_sentence_index_sorted(w2s_idx, word_rectangles):\n",
        "    new_idx = sorted(w2s_idx, key=lambda x: (x[1], word_rectangles[:, 0, 0][x[0]]))\n",
        "\n",
        "    return new_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEjvIZxyZeS0"
      },
      "source": [
        "####### Craft Dataset\n",
        "import string\n",
        "\n",
        "############################################\n",
        "########## ХЗ может это и не нужно##########\n",
        "############################################\n",
        "class CustomsDataset(Dataset):\n",
        "    def __init__(self, array_of_cutted_images, opt):\n",
        "        self.opt = opt\n",
        "        self.array_of_cutted_images = array_of_cutted_images\n",
        "    def __len__(self):\n",
        "        return len(self.array_of_cutted_images)\n",
        "    def __getitem__(self, index):\n",
        "        # (images, index_images) index_images = string/int - identifyer\n",
        "        #return (self.array_of_cutted_images[index], index)\n",
        "        if self.opt.rgb:\n",
        "            img = Image.fromarray(self.array_of_cutted_images[index]).convert('RGB')  # for color image\n",
        "        else:\n",
        "            img = Image.fromarray(self.array_of_cutted_images[index]).convert('L')\n",
        "        # print(f'Getted image for index {index}')\n",
        "        return (img, str(index))\n",
        "\n",
        "def recognition_pipeline(opt, word_cutted_images_list, debug=False):\n",
        "    import string\n",
        "    import argparse\n",
        "\n",
        "    import torch\n",
        "    import torch.backends.cudnn as cudnn\n",
        "    import torch.utils.data\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    from utils import CTCLabelConverter, AttnLabelConverter\n",
        "    from dataset import RawDataset, AlignCollate\n",
        "    from model import Model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \"\"\" model configuration \"\"\"\n",
        "    if 'CTC' in opt.Prediction:\n",
        "        converter = CTCLabelConverter(opt.character)\n",
        "    else:\n",
        "        converter = AttnLabelConverter(opt.character)\n",
        "    opt.num_class = len(converter.character)\n",
        "\n",
        "    if opt.rgb:\n",
        "        opt.input_channel = 3\n",
        "    model = Model(opt)\n",
        "    if debug:\n",
        "        print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
        "            opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
        "            opt.SequenceModeling, opt.Prediction)\n",
        "    model = torch.nn.DataParallel(model).to(device)\n",
        "\n",
        "    if debug:\n",
        "        print('Device %s' % device)\n",
        "\n",
        "    # load model\n",
        "    if debug:\n",
        "        print('loading pretrained model from %s' % opt.saved_model)\n",
        "\n",
        "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
        "\n",
        "    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
        "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
        "    #demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
        "    demo_data = CustomsDataset(word_cutted_images_list, opt=opt)\n",
        "    if debug:\n",
        "        print('CustomsDataset init')\n",
        "    demo_loader = torch.utils.data.DataLoader(\n",
        "        demo_data, batch_size=opt.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=int(opt.workers),\n",
        "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
        "    # print('demo_loader create')\n",
        "    # predict\n",
        "    model.eval()\n",
        "\n",
        "    result = []\n",
        "    with torch.no_grad():\n",
        "        for image_tensors, image_path_list in demo_loader:\n",
        "            # print(image_path_list)\n",
        "            batch_size = image_tensors.size(0)\n",
        "            image = image_tensors.to(device)\n",
        "            # For max length prediction\n",
        "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
        "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
        "\n",
        "            if 'CTC' in opt.Prediction:\n",
        "                preds = model(image, text_for_pred)\n",
        "\n",
        "                # Select max probabilty (greedy decoding) then decode index to character\n",
        "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
        "                _, preds_index = preds.max(2)\n",
        "                # preds_index = preds_index.view(-1)\n",
        "                preds_str = converter.decode(preds_index, preds_size)\n",
        "\n",
        "            else:\n",
        "                preds = model(image, text_for_pred, is_train=False)\n",
        "\n",
        "                # select max probabilty (greedy decoding) then decode index to character\n",
        "                _, preds_index = preds.max(2)\n",
        "                preds_str = converter.decode(preds_index, length_for_pred)\n",
        "\n",
        "\n",
        "            log = open(f'./log_demo_result.txt', 'a')\n",
        "            dashed_line = '-' * 80\n",
        "            head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\tconfidence score'\n",
        "            \n",
        "            # print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
        "            log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
        "\n",
        "            preds_prob = F.softmax(preds, dim=2)\n",
        "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
        "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
        "                if 'Attn' in opt.Prediction:\n",
        "                    pred_EOS = pred.find('[s]')\n",
        "                    pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
        "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
        "\n",
        "                # calculate confidence score (= multiply of pred_max_prob)\n",
        "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
        "\n",
        "                # print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')\n",
        "                log.write(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\\n')\n",
        "                result.append((pred, confidence_score.item()))\n",
        "            log.close()\n",
        "            return result\n",
        "\n",
        "def create_word_recongnition_args(is_use_second_model=True, is_sensitive=False):\n",
        "    opt = argparse.Namespace()\n",
        "    opt.trained_model = 'weights/craft_mlt_25k.pth'\n",
        "    opt.image_folder = \"demo_image/\" # NOT USED, DUMP\n",
        "    opt.workers = 4\n",
        "    opt.batch_size = 192\n",
        "    opt.character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
        "    # if is_use_second_model:\n",
        "    #     opt.sensitive = True\n",
        "    # else:\n",
        "    #     opt.sensitive = False\n",
        "\n",
        "    opt.sensitive = is_sensitive\n",
        "\n",
        "    if is_use_second_model:\n",
        "        if opt.sensitive:\n",
        "            opt.saved_model = \"weights/TPS-ResNet-BiLSTM-Attn-case-sensitive.pth\" # !!!\n",
        "        else:\n",
        "            opt.saved_model = \"weights/TPS-ResNet-BiLSTM-Attn.pth\" # !!!\n",
        "    else:\n",
        "        opt.saved_model = \"weights/TPS-ResNet-BiLSTM-CTC.pth\" # !!!\n",
        "        # \"\"\" Data processing \"\"\"\n",
        "    opt.batch_max_length = 25\n",
        "    opt.imgH = 32 \n",
        "    opt.imgW = 100\n",
        "    opt.rgb = False \n",
        "    opt.PAD = True\n",
        "    # \"\"\" Model Architecture \"\"\"\n",
        "\n",
        "    opt.Transformation = 'TPS'\n",
        "    opt.FeatureExtraction = 'ResNet'\n",
        "    opt.SequenceModeling = 'BiLSTM'\n",
        "    if is_use_second_model:\n",
        "        opt.Prediction = \"Attn\"\n",
        "    else:\n",
        "        opt.Prediction = \"CTC\"\n",
        "    opt.num_fiducial = 20\n",
        "    opt.input_channel = 1\n",
        "    opt.output_channel = 512\n",
        "    opt.hidden_size = 256\n",
        "    \"\"\" vocab / character number configuration \"\"\"\n",
        "    if opt.sensitive:\n",
        "        opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    cudnn.deterministic = True\n",
        "    opt.num_gpu = torch.cuda.device_count()\n",
        "\n",
        "    return opt\n",
        "\n",
        "def create_final_recognition_result(recognition_result_first, recognition_result_second):\n",
        "    final_result_list = []\n",
        "    for i in range(len(recognition_result_first)):\n",
        "        if recognition_result_first[i][1] > recognition_result_second[i][1]:\n",
        "            final_result_list.append(recognition_result_first[i])\n",
        "        else:\n",
        "            final_result_list.append(recognition_result_second[i])\n",
        "        #print(recognition_result_first[i][0], recognition_result_second[i][0])\n",
        "    #print(final_result_list)\n",
        "    return final_result_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVufm8XPaZME"
      },
      "source": [
        "##########################################################################\n",
        "###############Удаление текста с картинки#################################\n",
        "##########################################################################\n",
        "\n",
        "############ Object remover\n",
        "# import os\n",
        "# import glob\n",
        "# import torch\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torchvision.transforms.functional as F\n",
        "# from torch.utils.data import DataLoader\n",
        "# from PIL import Image\n",
        "# from imageio import imread\n",
        "# from skimage.feature import canny\n",
        "# from skimage.color import rgb2gray, gray2rgb\n",
        "# #from .utils import create_mask\n",
        "# from src.utils import create_mask\n",
        "# import cv2\n",
        "# #from .segmentor_fcn import segmentor,fill_gaps\n",
        "# from src.segmentor_fcn import segmentor,fill_gaps\n",
        "\n",
        "\n",
        "# class DatasetForRemoveWithMask(torch.utils.data.Dataset):\n",
        "#     def __init__(self, config, flist, edge_flist, image_data_list, augment=True, training=True, debug=True):\n",
        "#         super(DatasetForRemoveWithMask, self).__init__()\n",
        "#         self.augment = augment\n",
        "#         self.training = training\n",
        "#         self.data = self.load_flist(flist)\n",
        "#         self.edge_data = self.load_flist(edge_flist)\n",
        "\n",
        "#         self.debug = debug\n",
        "\n",
        "#         self.input_size = config.INPUT_SIZE\n",
        "#         self.sigma = config.SIGMA\n",
        "#         self.edge = config.EDGE\n",
        "#         self.mask = config.MASK\n",
        "#         self.nms = config.NMS\n",
        "#         self.device = config.SEG_DEVICE\n",
        "#         self.objects = config.OBJECTS\n",
        "#         self.segment_net = config.SEG_NETWORK\n",
        "#         # in test mode, there's a one-to-one relationship between mask and image\n",
        "#         # masks are loaded non random\n",
        "        \n",
        "#         # Наш массив с инфой\n",
        "#         self.image_data_list = image_data_list\n",
        "\n",
        "#     def __len__(self):\n",
        "#         #######################################################################\n",
        "#         #######################################################################\n",
        "#         #######################################################################\n",
        "#         #######################################################################\n",
        "#         # НЕ ЗАБЫТЬ ПОМЕНЯТЬ НА ПРАВИЛЬНОЕ ЗНАЧЕНИЕ\n",
        "#         # Если у будем обрабатывать более 1ой картинки, то\n",
        "#         # len(self.image_data_list[0][\"word_bboxes\"]) > 0\n",
        "#         #######################################################################\n",
        "#         #######################################################################\n",
        "#         #######################################################################\n",
        "#         #######################################################################\n",
        "#         return len(self.image_data_list[0][\"word_bboxes\"])\n",
        "#         #return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         try:\n",
        "#             item = self.load_item(index)\n",
        "#         except:\n",
        "#             print(\"Load image - \", self.image_data_list[index][\"image_path\"])\n",
        "#             item = self.load_item(index)\n",
        "\n",
        "#         return item\n",
        "\n",
        "#     def load_name(self, index):\n",
        "#         return self.image_data_list[index][\"image_file_name\"]\n",
        "        \n",
        "#     def load_size(self, index):\n",
        "#         # ХАРДКОД, модель не умеет работать с большими размерами\n",
        "#         # Только с картинками 256 на 256\n",
        "#         return 256, 256 \n",
        "\n",
        "# #######################################################################\n",
        "#     def centroid(self, vertexes):\n",
        "#         _x_list = [vertex [0] for vertex in vertexes]\n",
        "#         _y_list = [vertex [1] for vertex in vertexes]\n",
        "#         _len = len(vertexes)\n",
        "#         _x = sum(_x_list) / _len\n",
        "#         _y = sum(_y_list) / _len\n",
        "#         return(_x, _y)\n",
        "\n",
        "#     def create_cutted_images_list_from_rectangle(self, image_array, rectangle):\n",
        "#         # display(Image.fromarray(np.array(mask)))\n",
        "#         #x = rectangle.astype(int)\n",
        "#         x = rectangle\n",
        "\n",
        "#         if self.debug:\n",
        "#             print(x)\n",
        "#             print((x[1][0] - x[0][0]))\n",
        "#             print((x[1][1] - x[0][1]))\n",
        "\n",
        "#         if np.ndim(image_array) > 2:\n",
        "#             cutted_image =  image_array[\n",
        "#             x[0][1]:x[1][1], \n",
        "#             x[0][0]:x[1][0], \n",
        "#             :\n",
        "#             ]\n",
        "#         else:\n",
        "#             cutted_image =  image_array[\n",
        "#             x[0][1]:x[1][1], \n",
        "#             x[0][0]:x[1][0]\n",
        "#             ]\n",
        "        \n",
        "#         if self.debug:\n",
        "#             print(cutted_image.shape)\n",
        "#         return cutted_image\n",
        "# #######################################################################\n",
        "\n",
        "#     def load_item(self, index):\n",
        "\n",
        "#         size = self.input_size\n",
        "\n",
        "#         full_image = self.image_data_list[0][\"image_with_deleted_text\"]\n",
        "#         full_mask = self.image_data_list[0][\"mask_array_from_words\"]\n",
        "\n",
        "#         ################# CENTROID BOX ###################\n",
        "#         box_points_list = self.image_data_list[0]['word_bboxes'][index]\n",
        "\n",
        "#         center_point_x, center_point_y = self.centroid(box_points_list)\n",
        "\n",
        "#         left_rect_point = center_point_x - size / 2\n",
        "#         right_rect_point = center_point_x + size / 2\n",
        "#         if left_rect_point < 0: # Левый край за нулем, смещаем rect вправо\n",
        "#             right_rect_point -= left_rect_point \n",
        "#             left_rect_point = 0\n",
        "        \n",
        "#         if right_rect_point > self.image_data_list[0]['image_width']: # Левый край за нулем, смещаем rect вправо\n",
        "#             width_delta = right_rect_point - self.image_data_list[0]['image_width']\n",
        "#             right_rect_point -= width_delta \n",
        "#             left_rect_point -= width_delta \n",
        "\n",
        "\n",
        "#         top_rect_point = center_point_y - size / 2\n",
        "#         bottom_rect_point = center_point_y + size / 2\n",
        "#         if top_rect_point < 0: # Левый край за нулем, смещаем rect вправо\n",
        "#             bottom_rect_point -= top_rect_point \n",
        "#             top_rect_point = 0\n",
        "\n",
        "#         if bottom_rect_point > self.image_data_list[0]['image_height']: # Левый край за нулем, смещаем rect вправо\n",
        "#             height_delta = bottom_rect_point - self.image_data_list[0]['image_height']\n",
        "#             bottom_rect_point -= height_delta \n",
        "#             top_rect_point -= height_delta \n",
        "\n",
        "\n",
        "#         centroid_image_rectangle = [[left_rect_point, top_rect_point], [right_rect_point, bottom_rect_point]]\n",
        "#         centroid_image_rectangle = np.array(centroid_image_rectangle)\n",
        "#         centroid_image_rectangle = centroid_image_rectangle.astype(int)\n",
        "\n",
        "#         cutted_image = self.create_cutted_images_list_from_rectangle(full_image, centroid_image_rectangle)\n",
        "#         cutted_mask = self.create_cutted_images_list_from_rectangle(full_mask, centroid_image_rectangle)\n",
        "#         ################# CENTROID BOX ###################\n",
        "\n",
        "#         if self.debug:    \n",
        "#             print(\"Load image - \", self.image_data_list[0][\"image_path\"])\n",
        "#         # img = Image.fromarray(self.image_data_list[0][\"image_array\"])\n",
        "#         #img = img.resize((size, size), Image.ANTIALIAS)\n",
        "#         img = cutted_image\n",
        "        \n",
        "#         # gray to rgb\n",
        "#         if Image.fromarray(img).mode !='RGB':\n",
        "#             img = gray2rgb(img)\n",
        "\n",
        "#         if self.debug:\n",
        "#             print(\"img shape -\", img.shape)\n",
        "#             print(\"img type -\", type(img))\n",
        "\n",
        "#         mask = cutted_mask\n",
        "\n",
        "#         # НАЛОЖЕНИЕ МАСКИ\n",
        "#         # В маске есть \"горящие\" пиксели, которые нужно удалить и заменить\n",
        "#         # дорисовать на их место фон. Мы выдираем пиксели\n",
        "#         idx=(mask>0)\n",
        "#         mask[idx]=255\n",
        "\n",
        "#         # create grayscale image\n",
        "#         img_gray = rgb2gray(np.array(img))\n",
        "\n",
        "\n",
        "#         # load edge\n",
        "#         edge = self.load_edge(img_gray, index, mask)\n",
        "\n",
        "#         # augment data\n",
        "#         if self.augment and np.random.binomial(1, 0.5) > 0:\n",
        "#             img = img[:, ::-1, ...]\n",
        "#             img_gray = img_gray[:, ::-1, ...]\n",
        "#             edge = edge[:, ::-1, ...]\n",
        "#             mask = mask[:, ::-1, ...]\n",
        "\n",
        "#         return self.image_to_tensor(img), self.image_to_tensor(img_gray), self.image_to_tensor(edge), self.image_to_tensor(mask), F.to_tensor(centroid_image_rectangle).float(), cutted_image\n",
        "\n",
        "#     def load_edge(self, img, index, mask):\n",
        "#         sigma = self.sigma\n",
        "\n",
        "#         # in test mode images are masked (with masked regions),\n",
        "#         # using 'mask' parameter prevents canny to detect edges for the masked regions\n",
        "#         mask = None if self.training else (1 - mask / 255).astype(np.bool)\n",
        "        \n",
        "#         # canny\n",
        "#         if self.edge == 1:\n",
        "#             # no edge\n",
        "#             if sigma == -1:\n",
        "#                 return np.zeros(img.shape).astype(np.float)\n",
        "\n",
        "#             # random sigma\n",
        "#             if sigma == 0:\n",
        "#                 sigma = random.randint(1, 4)\n",
        "\n",
        "#             return canny(img, sigma=sigma, mask=mask).astype(np.float)\n",
        "\n",
        "#         # external\n",
        "#         else:\n",
        "#             imgh, imgw = img.shape[0:2]\n",
        "#             edge = imread(self.edge_data[index])\n",
        "#             edge = self.resized(edge, imgh, imgw)\n",
        "\n",
        "#             # non-max suppression\n",
        "#             if self.nms == 1:\n",
        "#                 edge = edge * canny(img, sigma=sigma, mask=mask)\n",
        "\n",
        "#             return edge\n",
        "\n",
        "    \n",
        "#     def image_to_tensor(self, img):\n",
        "#         img = Image.fromarray(img)\n",
        "#         img_t = F.to_tensor(img).float()\n",
        "#         return img_t\n",
        "\n",
        "#     def load_flist(self, flist):\n",
        "#         if isinstance(flist, list):\n",
        "#             return flist\n",
        "\n",
        "#         # flist: image file path, image directory path, text file flist path\n",
        "#         if isinstance(flist, str):\n",
        "#             if os.path.isdir(flist):\n",
        "#                 flist = list(glob.glob(flist + '/*.jpg')) + list(glob.glob(flist + '/*.png'))\n",
        "#                 flist.sort()\n",
        "#                 return flist\n",
        "\n",
        "#             if os.path.isfile(flist):\n",
        "#                 try:\n",
        "#                     return np.genfromtxt(flist, dtype=np.str, encoding='utf-8')\n",
        "#                 except:\n",
        "#                     return [flist]\n",
        "\n",
        "#         return []\n",
        "\n",
        "#     def create_iterator(self, batch_size):\n",
        "#         while True:\n",
        "#             sample_loader = DataLoader(\n",
        "#                 dataset=self,\n",
        "#                 batch_size=batch_size,\n",
        "#                 drop_last=True\n",
        "#             )\n",
        "\n",
        "#             for item in sample_loader:\n",
        "#                 yield item\n",
        "\n",
        "#     def postprocess(self, img):\n",
        "#         # [0, 1] => [0, 255]\n",
        "#         img = img * 255.0\n",
        "#         #img = img.permutation(0, 2, 3, 1)\n",
        "#         #return img.astype(int)\n",
        "#         return img\n",
        "\n",
        "# '''\n",
        "#         Code of EdgeConnect is from this repo\n",
        "#         https://github.com/knazeri/edge-connect\n",
        "#         '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class EdgeConnectNew():\n",
        "#     def __init__(self, config, image_data_list, debug=False):\n",
        "\n",
        "#         self.config = config\n",
        "        \n",
        "\n",
        "#         if config.MODEL == 1:\n",
        "#             model_name = 'edge'\n",
        "#         elif config.MODEL == 2:\n",
        "#             model_name = 'inpaint'\n",
        "#         elif config.MODEL == 3:\n",
        "#             model_name = 'edge_inpaint'\n",
        "#         elif config.MODEL == 4:\n",
        "#             model_name = 'joint'\n",
        "\n",
        "#         #self.debug = True\n",
        "#         self.debug = debug\n",
        "#         self.model_name = model_name\n",
        "#         self.edge_model = EdgeModel(config).to(config.DEVICE)\n",
        "#         self.inpaint_model = InpaintingModel(config).to(config.DEVICE)\n",
        "#         self.image_data_list = image_data_list\n",
        "#         if self.debug:\n",
        "#             print(\"--------------\")\n",
        "#             print(\"config.MODEL=\", config.MODEL)\n",
        "#             print(config.TEST_FLIST)\n",
        "#             print(config.TEST_EDGE_FLIST)\n",
        "#             print(self.model_name)\n",
        "#             print(config)\n",
        "#             print(\"--------------/\")\n",
        "#         # test mode\n",
        "#         # config.TEST_FLIST - Путь до папки с файлами с картинками\n",
        "#         self.test_dataset = DatasetForRemoveWithMask(\n",
        "#             config, \n",
        "#             config.TEST_FLIST, \n",
        "#             config.TEST_EDGE_FLIST, \n",
        "#             image_data_list=self.image_data_list, \n",
        "#             augment=False, \n",
        "#             training=False,\n",
        "#             debug=debug\n",
        "#         )\n",
        "\n",
        "#         self.samples_path = os.path.join(config.PATH, 'samples')\n",
        "#         self.results_path = os.path.join(config.PATH, 'results')\n",
        "\n",
        "#         if config.RESULTS is not None:\n",
        "#             self.results_path = os.path.join(config.RESULTS)\n",
        "\n",
        "#         if config.DEBUG is not None and config.DEBUG != 0:\n",
        "#             self.debug = True\n",
        "\n",
        "#         self.log_file = os.path.join(config.PATH, 'log_' + model_name + '.dat')\n",
        "\n",
        "#     def load(self):\n",
        "#         if self.config.MODEL == 1:\n",
        "#             self.edge_model.load()\n",
        "\n",
        "#         elif self.config.MODEL == 2:\n",
        "#             self.inpaint_model.load()\n",
        "\n",
        "#         else:\n",
        "#             self.edge_model.load()\n",
        "#             self.inpaint_model.load()\n",
        "\n",
        "#     def save(self):\n",
        "#         if self.config.MODEL == 1:\n",
        "#             self.edge_model.save()\n",
        "\n",
        "#         elif self.config.MODEL == 2 or self.config.MODEL == 3:\n",
        "#             self.inpaint_model.save()\n",
        "\n",
        "#         else:\n",
        "#             self.edge_model.save()\n",
        "#             self.inpaint_model.save()\n",
        "\n",
        "\n",
        "#     def test(self):\n",
        "#         self.edge_model.eval()\n",
        "#         self.inpaint_model.eval()\n",
        "\n",
        "#         model = self.config.MODEL\n",
        "#         create_dir(self.results_path)\n",
        "\n",
        "#         test_loader = DataLoader(\n",
        "#             dataset=self.test_dataset,\n",
        "#             batch_size=1,\n",
        "#         )\n",
        "\n",
        "#     def test(self):\n",
        "#         self.edge_model.eval()\n",
        "#         self.inpaint_model.eval()\n",
        "\n",
        "#         model = self.config.MODEL\n",
        "#         create_dir(self.results_path)\n",
        "\n",
        "#         test_loader = DataLoader(\n",
        "#             dataset=self.test_dataset,\n",
        "#             batch_size=1,\n",
        "#         )\n",
        "\n",
        "#         index = 0\n",
        "#         index_tmp = 0\n",
        "#         for items in test_loader:\n",
        "#             name = self.test_dataset.load_name(index) # Имя картинки которую обрабатываем\n",
        "\n",
        "        \n",
        "#             # Загруженная картинкаб сервый цвет\n",
        "#             images, images_gray, edges, masks, centroid_image_rectangle, cutted_image = self.cuda(*items) \n",
        "            \n",
        "#             ######################################################\n",
        "#             ######################################################\n",
        "#             # Проверка, не полностью ли у нас уже обработана маска (попали второй раз на тоже место)\n",
        "#             current_mask = np.array(self.postprocess(masks)[0].cpu().numpy().astype(np.uint8))\n",
        "#             current_mask = np.squeeze(current_mask) # Уменьшаем размерность\n",
        "            \n",
        "#             # В данной маске есть \"пиксели для замены\"\n",
        "#             idx=(current_mask>0)\n",
        "            \n",
        "#             # Все индексы в данной маске = True, т.е. на этой картинке нам ничего закрашивать не нужно\n",
        "#             if self.debug:\n",
        "#                 print(\"idx.size == np.sum(idx)\", idx.size, np.sum(idx))\n",
        "#             #if idx.size == np.sum(idx):\n",
        "#             if np.sum(idx) == 0:\n",
        "#                 index_tmp += 1\n",
        "#                 continue\n",
        "\n",
        "#             # Проверка, не полностью ли у нас уже обработана маска (попали второй раз на тоже место)\n",
        "#             ######################################################\n",
        "#             ######################################################\n",
        "\n",
        "#             # edge model\n",
        "#             if model == 1:\n",
        "#                 outputs = self.edge_model(images_gray, edges, masks)\n",
        "#                 outputs_merged = (outputs * masks) + (edges * (1 - masks))\n",
        "\n",
        "#             # inpaint model\n",
        "#             elif model == 2:\n",
        "#                 outputs = self.inpaint_model(images, edges, masks)\n",
        "#                 outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
        "\n",
        "#             # inpaint with edge model / joint model\n",
        "#             else:\n",
        "#                 edges = self.edge_model(images_gray, edges, masks).detach()\n",
        "#                 outputs = self.inpaint_model(images, edges, masks)\n",
        "#                 outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
        "\n",
        "#             output = self.postprocess(outputs_merged)[0]            \n",
        "#             path = os.path.join(self.results_path, name)\n",
        "#             if self.debug:\n",
        "#                 print(index, name)\n",
        "\n",
        "#             imsave(output, path)\n",
        "\n",
        "#             ############################################################################################################\n",
        "#             ############ Заменяем пиксели в нашей старой картинке на новые пиксели ############\n",
        "#             images_with_delete_text = output.cpu().numpy().astype(np.uint8)\n",
        "#             cutted_image = cutted_image[0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "#             cutted_image[idx] = images_with_delete_text[idx]\n",
        "#             display(Image.fromarray(cutted_image))\n",
        "\n",
        "#             #######################################################################\n",
        "#             #######################################################################\n",
        "#             #######################################################################\n",
        "#             #######################################################################\n",
        "#             # НЕ ЗАБЫТЬ ПОМЕНЯТЬ НА ПРАВИЛЬНОЕ ЗНАЧЕНИЕ\n",
        "#             # Если у будем обрабатывать более 1ой картинки, то\n",
        "#             # image_data_list[0] - нужно заменить на соответсвующий индекс\n",
        "#             #######################################################################\n",
        "#             #######################################################################\n",
        "#             #######################################################################\n",
        "#             #######################################################################\n",
        "#             centroid_coordinate = centroid_image_rectangle.cpu().numpy().squeeze()\n",
        "#             self.image_data_list[0][\"image_with_deleted_text\"][\n",
        "#                 int(centroid_coordinate[0][1]):int(centroid_coordinate[1][1]), \n",
        "#                 int(centroid_coordinate[0][0]):int(centroid_coordinate[1][0]), \n",
        "#                 :\n",
        "#             ][idx] = cutted_image[idx]\n",
        "\n",
        "#             self.image_data_list[0][\"mask_array_from_words\"][\n",
        "#                 int(centroid_coordinate[0][1]):int(centroid_coordinate[1][1]), \n",
        "#                 int(centroid_coordinate[0][0]):int(centroid_coordinate[1][0])\n",
        "#             ][idx] = 0\n",
        "\n",
        "#             if self.debug:\n",
        "#                 display(Image.fromarray(self.image_data_list[0][\"image_with_deleted_text\"]))\n",
        "#                 display(Image.fromarray(self.image_data_list[0][\"mask_array_from_words\"]))\n",
        "#             ############ Заменяем пиксели в нашей старой картинке на новые пиксели ############\n",
        "#             ############################################################################################################\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "#             if self.debug:\n",
        "#                 print(images.shape)\n",
        "#                 print(masks.shape)\n",
        "#                 print(edges.shape)\n",
        "\n",
        "#                 edges = self.postprocess(1 - edges)[0]\n",
        "#                 masked = self.postprocess(images * (1 - masks) + masks)[0]\n",
        "#                 fname, fext = name.split('.')\n",
        "\n",
        "#                 imsave(edges, os.path.join(self.results_path, fname + '_edge.' + fext))\n",
        "#                 imsave(masked, os.path.join(self.results_path, fname + '_masked.' + fext))\n",
        "\n",
        "#             #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#             #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#             #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#             #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#             #index += 1\n",
        "#             index_tmp += 1\n",
        "\n",
        "#         if self.debug:\n",
        "#             print('\\nEnd test....')\n",
        "\n",
        "\n",
        "#     def log(self, logs):\n",
        "#         with open(self.log_file, 'a') as f:\n",
        "#             f.write('%s\\n' % ' '.join([str(item[1]) for item in logs]))\n",
        "\n",
        "#     def cuda(self, *args):\n",
        "#         return (item.to(self.config.DEVICE) for item in args)\n",
        "\n",
        "#     def postprocess(self, img):\n",
        "#         # [0, 1] => [0, 255]\n",
        "#         img = img * 255.0\n",
        "#         img = img.permute(0, 2, 3, 1)\n",
        "#         return img.int()\n",
        "\n",
        "\n",
        "####################################################################\n",
        "####################################################################\n",
        "####################################################################\n",
        "# ######## Object remover MAIN\n",
        "# import os\n",
        "# import cv2\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import argparse\n",
        "# from shutil import copyfile\n",
        "# from src.config import Config\n",
        "# #from src.edge_connect import EdgeConnect\n",
        "\n",
        "\n",
        "# def main(image_data_list, mode=None, debug=False):\n",
        "#     r\"\"\"starts the model\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     config = load_object_remover_config(mode)\n",
        "\n",
        "\n",
        "#     # cuda visble devices\n",
        "#     os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(e) for e in config.GPU)\n",
        "\n",
        "\n",
        "#     # init device\n",
        "#     if torch.cuda.is_available():\n",
        "#         config.DEVICE = torch.device(\"cuda\")\n",
        "#         torch.backends.cudnn.benchmark = True   # cudnn auto-tuner\n",
        "#     else:\n",
        "#         config.DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "#     # set cv2 running threads to 1 (prevents deadlocks with pytorch dataloader)\n",
        "#     cv2.setNumThreads(0)\n",
        "\n",
        "\n",
        "#     # initialize random seed\n",
        "#     torch.manual_seed(config.SEED)\n",
        "#     torch.cuda.manual_seed_all(config.SEED)\n",
        "#     np.random.seed(config.SEED)\n",
        "#     random.seed(config.SEED)\n",
        "\n",
        "\n",
        "\n",
        "#     # build the model and initialize\n",
        "#     model = EdgeConnectNew(config, image_data_list)\n",
        "#     model.load()\n",
        "\n",
        "\n",
        "    \n",
        "#     # model test\n",
        "#     if debug:\n",
        "#         print('\\nstart testing...\\n')\n",
        "#     model.test()\n",
        "\n",
        "    \n",
        "\n",
        "# def load_object_remover_config(mode=None):\n",
        "#     r\"\"\"loads model config\n",
        "\n",
        "#     \"\"\"\n",
        "#     # parser = argparse.ArgumentParser()\n",
        "#     # parser.add_argument('--path', '--checkpoints', type=str, default='./checkpoints', help='model checkpoints path (default: ./checkpoints)')\n",
        "#     # parser.add_argument('--model', type=int, choices=[1, 2, 3, 4], help='1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model')\n",
        "\n",
        "#     # # test mode\n",
        "#     # parser.add_argument('--input', type=str, help='path to the input images directory or an input image')\n",
        "#     # parser.add_argument('--edge', type=str, help='path to the edges directory or an edge file')\n",
        "#     # parser.add_argument('--output', type=str, help='path to the output directory')\n",
        "#     # parser.add_argument('--remove', nargs= '*' ,type=int, help='objects to remove')\n",
        "#     # parser.add_argument('--cpu', type=str, help='machine to run segmentation model on')\n",
        "#     # args = parser.parse_args()\n",
        "    \n",
        "#     #!python test.py --input ./examples/my_small_data --output ./checkpoints/resultsfinal --remove 3 15\n",
        "\n",
        "#     args = argparse.Namespace()\n",
        "#     args.path = './checkpoints' # type=str, default='./checkpoints', help='model checkpoints path (default: ./checkpoints)')\n",
        "#     args.model = 3 #', type=int, choices=[1, 2, 3, 4], help='1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model')\n",
        "\n",
        "#     # # test mode\n",
        "#     args.input = './examples/my_small_data' #, type=str, help='path to the input images directory or an input image')\n",
        "#     args.edge = None # ', type=str, help='path to the edges directory or an edge file')\n",
        "#     args.output = './checkpoints/resultsfinal' #', type=str, help='path to the output directory')\n",
        "#     args.remove = [3,15] # ', nargs= '*' ,type=int, help='objects to remove')\n",
        "#     args.cpu  = None #', type=str, help='machine to run segmentation model on')\n",
        "\n",
        "\n",
        "#     #if path for checkpoint not given\n",
        "#     if args.path is None:\n",
        "#         args.path='./checkpoints'\n",
        "#     config_path = os.path.join(args.path, 'config.yml')\n",
        "    \n",
        "#        # create checkpoints path if does't exist\n",
        "#     if not os.path.exists(args.path):\n",
        "#         os.makedirs(args.path)\n",
        "\n",
        "#     # copy config template if does't exist\n",
        "#     if not os.path.exists(config_path):\n",
        "#         copyfile('./Automated-objects-removal-inpainter/config.yml.example', config_path)\n",
        "\n",
        "#     # load config file\n",
        "#     config = Config(config_path)\n",
        "\n",
        "   \n",
        "#     # test mode\n",
        "#     config.MODE = 2\n",
        "#     config.MODEL = args.model if args.model is not None else 3\n",
        "#     config.OBJECTS = args.remove if args.remove is not None else [3,15]\n",
        "#     config.SEG_DEVICE = 'cpu' if args.cpu is not None else 'cuda'\n",
        "#     config.INPUT_SIZE = 256\n",
        "#     if args.input is not None:\n",
        "#         config.TEST_FLIST = args.input\n",
        "    \n",
        "#     if args.edge is not None:\n",
        "#         # Дефолтное значение\n",
        "#         # TEST_EDGE_FLIST: ./datasets/places2_edges_test.flist\n",
        "#         config.TEST_EDGE_FLIST = args.edge\n",
        "#     if args.output is not None:\n",
        "#         config.RESULTS = args.output\n",
        "#     else: \n",
        "#         if not os.path.exists('./results_images'):\n",
        "#             os.makedirs('./results_images')\n",
        "#         config.RESULTS = './results_images'\n",
        "    \n",
        "    \n",
        "      \n",
        "    \n",
        "    \n",
        "#     return config\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZyR2Hwn9a7r"
      },
      "source": [
        "# def pipeline(image_link, debug=True):\n",
        "    \n",
        "#     image_path = image_link\n",
        "#     image_file_name = os.path.basename(image_path)\n",
        "#     image_pil = Image.open(BytesIO(requests.get(image_path).content))\n",
        "#     source_image_for_output = image_pil.copy() # Исходная картинка которую мы подадим на выход для сравнения\n",
        "#     if debug:\n",
        "#         display('downloaded image', image_pil)\n",
        "#     image_array = np.array(image_pil)\n",
        "\n",
        "#     image_width = image_array.shape[1]\n",
        "#     image_height = image_array.shape[0]\n",
        "#     if debug:\n",
        "#         print(\"Image width = \", image_width, \"Image hight = \", image_height)\n",
        "\n",
        "#     args = create_craft_args(refine=False)\n",
        "#     word_bboxes, word_polys, word_score_text = create_text_mask(args, image_array)\n",
        "\n",
        "#     #args = create_craft_args(refine=True)\n",
        "#     #sentence_bboxes, sentence_polys, sentence_score_text = create_text_mask(args, image_array)\n",
        "\n",
        "#     mask_array_from_words = get_image_mask_from_boxes(image_array, word_bboxes)\n",
        "#     if debug:\n",
        "#         display(Image.fromarray(mask_array_from_words))\n",
        "\n",
        "#     word_rectangles = transform_bboxes_to_rectangles(word_bboxes)\n",
        "\n",
        "#     ## Print finded words, but can throw Error\n",
        "#     # if debug:\n",
        "#     #     word_cutted_images_list = create_cutted_images_list(image_array, word_rectangles)\n",
        "#     #     print('word_cutted_images_list', len(word_cutted_images_list))\n",
        "#     #     for i in range(len(word_cutted_images_list)):\n",
        "#     #         display(Image.fromarray(word_cutted_images_list[i]))\n",
        "\n",
        "#     image_data_dictionary = {\n",
        "#         \"image_path\": image_path,\n",
        "#         \"image_file_name\": image_file_name,\n",
        "#         \"image_array\": image_array.copy(),\n",
        "#         \"image_with_deleted_text\": image_array.copy(), # Картинка в которой будем \"удалять\" текст\n",
        "#         \"image_width\": image_width,\n",
        "#         \"image_height\": image_height, \n",
        "#         \"word_bboxes\": word_bboxes.copy(),\n",
        "#         \"word_rectangles\": word_rectangles,\n",
        "#         \"mask_array_from_words\": mask_array_from_words.copy()\n",
        "#     }\n",
        "#     image_data_list = [image_data_dictionary]\n",
        "\n",
        "#     # Удаление текста, mode это алгоритм который удаляет, второй это комбинированный из двух алгоритмов\n",
        "#     main(image_data_list, mode=2)\n",
        "\n",
        "#     image_with_deleted_text = Image.fromarray(image_data_list[0][\"image_with_deleted_text\"])\n",
        "\n",
        "#     return source_image_for_output, image_with_deleted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Font size detection\n",
        "#from transformers import FSMTTokenizer, FSMTForConditionalGeneration\n",
        "from shapely.geometry import Point, Polygon\n",
        "from shapely.ops import cascaded_union\n",
        "import PIL.ImageDraw as ImageDraw\n",
        "\n",
        "###################################################################\n",
        "#####Перевод и текста и рисование текста###########################\n",
        "###################################################################\n",
        "\n",
        "\n",
        "def search_font_size(text, bbox, debug=False):\n",
        "    width, height = bbox[2]-bbox[0]\n",
        "    font_size = 1\n",
        "    font = ImageFont.truetype(\"fonts/arial.ttf\", font_size)\n",
        "    font_width, font_height = font.getsize(text)\n",
        "    if debug:\n",
        "        print(width, height, font_width, font_height, font_size)\n",
        "    while font_width < width and font_height < height:\n",
        "        font_size += 1\n",
        "        font = ImageFont.truetype(\"fonts/arial.ttf\", font_size)\n",
        "        font_width, font_height = font.getsize(text)\n",
        "        if debug:\n",
        "            print(width, height, font_width, font_height, font_size)\n",
        "    font_size -= 1\n",
        "    if debug:\n",
        "        print(font_size)\n",
        "        print(width, height, font_width, font_height, font_size)\n",
        "    return ImageFont.truetype(\"fonts/arial.ttf\", font_size)\n",
        "\n",
        "# def compile_image(image, sentence_dict, sentence_bboxes):\n",
        "\n",
        "#     image = image.copy()\n",
        "#     draw = ImageDraw.Draw(image)\n",
        "\n",
        "#     for idx, sentence in sentence_dict.items():\n",
        "#         draw.text((sentence_bboxes[idx][0][0],sentence_bboxes[idx][0][1]), \n",
        "#                 sentence, \n",
        "#                 font=search_font_size(sentence, sentence_bboxes[idx]), \n",
        "#                 fill=(0, 0, 0), \n",
        "#                 stroke_width=2, \n",
        "#                 stroke_fill=(255,255,255))\n",
        "#     return image\n",
        "        \n",
        "\n",
        "def create_sentence_dict(w2s_idx_sorted, recognized_word_list):\n",
        "    result = {}\n",
        "\n",
        "    for el in w2s_idx_sorted:\n",
        "        if el[1] in result.keys():\n",
        "            result[el[1]].append(recognized_word_list[el[0]])\n",
        "        else:\n",
        "            result[el[1]] = []\n",
        "            result[el[1]].append(recognized_word_list[el[0]])\n",
        "\n",
        "    return result\n",
        "\n",
        "def translate_sentence(string, translator_model, translator_tokenizer):\n",
        "    input_ids = translator_tokenizer.encode(string, return_tensors=\"pt\")\n",
        "    outputs = translator_model.generate(input_ids)\n",
        "    decoded = translator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "def translate_sentence_dict(dict_, translator_model, translator_tokenizer, debug=False):\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for key, value in dict_.items():\n",
        "        if debug:\n",
        "            print(key, value)\n",
        "        result[key] = translate_sentence(' '.join(value), translator_model, translator_tokenizer)\n",
        "    return result\n",
        "\n",
        "def bounds_to_bbox(x):\n",
        "    return np.array([[x[0], x[1]], [x[2], x[1]], [x[2], x[3]], [x[0], x[3]]], dtype='float32')\n",
        "\n",
        "def display_image_boxes(image_array, boxes):\n",
        "    import PIL.ImageDraw as ImageDraw\n",
        "    import PIL.Image as Image\n",
        "    mask = Image.new(\"L\", (image_array.shape[1], image_array.shape[0]))\n",
        "    draw = ImageDraw.Draw(mask)\n",
        "    font = ImageFont.truetype(\"fonts/arial.ttf\", 12)\n",
        "    for i in range(len(boxes)):\n",
        "        draw.polygon(boxes[i], outline=256, fill=256)\n",
        "        draw.text((boxes[i][0][0],boxes[i][0][1]), \n",
        "            text=str(i), \n",
        "            font=font)\n",
        "    display(mask)\n",
        "\n",
        "def create_paragraph_bboxes(sentence_bboxes\n",
        "                            ):\n",
        "    bboxes = sorted(sentence_bboxes,key=lambda x: x[0][0])\n",
        "    paragraph_bboxes = []\n",
        "    counter = 0\n",
        "    # print('counter in ', counter)\n",
        "    indexes = {x: 0 for x in list(range(len(sentence_bboxes)))}\n",
        "    for num_master, box_master in enumerate(bboxes):\n",
        "        for num_slave, box_slave in enumerate(bboxes):\n",
        "            if indexes[num_master] == 0 and indexes[num_slave] == 0 and num_master != num_slave:\n",
        "                # print('master ', num_master, 'slave ', num_slave)\n",
        "                poly_master = Polygon(box_master)\n",
        "                poly_slave = Polygon(box_slave)\n",
        "                bounds_master = poly_master.bounds\n",
        "                bounds_slave = poly_slave.bounds\n",
        "                min_distance = poly_master.distance(poly_slave)\n",
        "\n",
        "                if bounds_slave[0] < bounds_master[3] and min_distance <=10:\n",
        "                    # print('append')\n",
        "                    paragraph_bboxes.append(bounds_to_bbox(cascaded_union([poly_master, poly_slave]).bounds))\n",
        "                    counter += 1\n",
        "                    indexes[num_master] = 1\n",
        "                    indexes[num_slave] = 1\n",
        "        if indexes[num_master] == 0:\n",
        "            paragraph_bboxes.append(box_master)\n",
        "    # print('counter out ', counter)\n",
        "    if counter == 0:\n",
        "        return sentence_bboxes\n",
        "    else:\n",
        "        return create_paragraph_bboxes(paragraph_bboxes)\n",
        "\n",
        "def create_word_2_sentence_index(word_bboxes, sentence_bboxes):\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for w_idx in range(len(word_bboxes)):\n",
        "        arr = np.array([])\n",
        "        for s_idx in range(len(sentence_bboxes)):\n",
        "            word_polygon = Polygon(word_bboxes[w_idx])\n",
        "            sent_polygon = Polygon(sentence_bboxes[s_idx])\n",
        "            share_intersection = word_polygon.intersection(sent_polygon).area/word_polygon.area\n",
        "            arr = np.append(arr, share_intersection)\n",
        "        \n",
        "        result[w_idx] = arr.argmax()\n",
        "        # print(arr.tolist())\n",
        "    \n",
        "    return list(result.items())\n",
        "\n",
        "def create_sentence_2_paragraph_index_sorted(sent2para_idx, sentence_bboxes):\n",
        "    new_idx = sorted(sent2para_idx, key=lambda x: (x[1], sentence_bboxes[:, 0, 1][x[0]]))\n",
        "\n",
        "    return new_idx\n",
        "\n",
        "def create_paragraph_dict(sent2para_index_sorted, sentence_dict):\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for el in sent2para_index_sorted:\n",
        "        if el[1] in result.keys():\n",
        "            result[el[1]]['list'] = result[el[1]]['list'] + sentence_dict[el[0]]\n",
        "            result[el[1]]['splits'] += 1\n",
        "        else:\n",
        "            result[el[1]] = {\n",
        "                'list': sentence_dict[el[0]],\n",
        "                'splits': 0\n",
        "            }\n",
        "    return result\n",
        "\n",
        "# def translate_sentence(string):\n",
        "#     input_ids = tokenizer.encode(string, return_tensors=\"pt\")\n",
        "#     outputs = model.generate(input_ids)\n",
        "#     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#     return decoded\n",
        "\n",
        "def translate_paragraph_dict(dict_, translator_model, translator_tokenizer, debug=False):\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for key in dict_.keys():\n",
        "        result[key] = translate_sentence(' '.join(dict_[key]['list']), translator_model, translator_tokenizer)\n",
        "        paragraph_array = result[key].split(' ')\n",
        "        step = 0 if dict_[key]['splits']==0 else math.ceil(len(paragraph_array)/dict_[key]['splits'])\n",
        "        if debug:\n",
        "            print('step ', step)\n",
        "        shuffle = 0\n",
        "        if step > 0:\n",
        "            for i in range(step, len(paragraph_array)+len(result[key])%step, step):\n",
        "                if debug:\n",
        "                    print(i)\n",
        "                paragraph_array.insert(i+shuffle, '\\n')\n",
        "                shuffle += 1\n",
        "        result[key] = ' '.join(paragraph_array)\n",
        "    return result\n",
        "\n",
        "def search_font_size(image, text, bbox, debug=False):\n",
        "    if debug:\n",
        "        print(bbox)\n",
        "    width, height = bbox[2]-bbox[0]\n",
        "    font_size = 1\n",
        "    font = ImageFont.truetype(\"fonts/arial.ttf\", font_size)\n",
        "    # font_width, font_height = font.getsize(text)\n",
        "    font_width, font_height = ImageDraw.ImageDraw(image).multiline_textsize(text, font=font)\n",
        "    if debug:\n",
        "        print('width {}, height {}, font_width {}, font_height {}, font_size {}'.format(width, height, font_width, font_height, font_size))\n",
        "    while font_width < width :# and font_height < height:\n",
        "        font_size += 1\n",
        "        font = ImageFont.truetype(\"fonts/arial.ttf\", font_size)\n",
        "        # font_width, font_height = font.getsize(text)\n",
        "        font_width, font_height = ImageDraw.ImageDraw(image).multiline_textsize(text, font=font)\n",
        "        if debug:\n",
        "            print('planned font size ', font.getsize(text))\n",
        "            print('width {}, height {}, font_width {}, font_height {}, font_size {}'.format(width, height, font_width, font_height, font_size))\n",
        "    font_size -= 1\n",
        "    if debug:\n",
        "        print('out font size ', font_size)\n",
        "        print('width {}, height {}, font_width {}, font_height {}, font_size {}'.format(width, height, font_width, font_height, font_size))\n",
        "    return ImageFont.truetype(\"fonts/arial.ttf\", font_size)\n",
        "\n",
        "def compile_image(image, sentence_dict, sentence_bboxes, debug):\n",
        "\n",
        "    image = image.copy()\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    for idx, sentence in sentence_dict.items():\n",
        "        # draw.polygon(sentence_bboxes[idx], outline=256, fill=256)\n",
        "        draw.text((sentence_bboxes[idx][0][0],sentence_bboxes[idx][0][1]), \n",
        "                sentence, \n",
        "                font=search_font_size(image, sentence, sentence_bboxes[idx], debug), \n",
        "                fill=(0, 0, 0), \n",
        "                stroke_width=2, \n",
        "                stroke_fill=(255,255,255))\n",
        "    return image\n",
        "        "
      ],
      "metadata": {
        "id": "dofIDxqMhqUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def pipeline(image_link, model_isr, model_translator, tokenizer_translator, font, debug=True):\n",
        "\n",
        "def pipeline(image_link, debug=True):\n",
        "\n",
        "    image_path = image_link\n",
        "    image_file_name = os.path.basename(image_path)\n",
        "    image_pil = Image.open(BytesIO(requests.get(image_path).content))\n",
        "    source_image_for_output = image_pil.copy() # Исходная картинка которую мы подадим на выход для сравнения\n",
        "    if debug:\n",
        "        display('downloaded image', image_pil)\n",
        "    image_array = np.array(image_pil)\n",
        "\n",
        "    image_width = image_array.shape[1]\n",
        "    image_height = image_array.shape[0]\n",
        "    if debug:\n",
        "        print(\"Image width = \", image_width, \"Image hight = \", image_height)\n",
        "\n",
        "    args = create_craft_args(refine=False)\n",
        "    word_bboxes, word_polys, word_score_text = create_text_mask(args, image_array)\n",
        "\n",
        "    args = create_craft_args(refine=True)\n",
        "    sentence_bboxes, sentence_polys, sentence_score_text = create_text_mask(args, image_array)\n",
        "\n",
        "    mask_array_from_words = get_image_mask_from_boxes(image_array, word_bboxes)\n",
        "\n",
        "    word_rectangles = transform_bboxes_to_rectangles(word_bboxes)\n",
        "    word_cutted_images_list = create_cutted_images_list(image_array, word_rectangles)\n",
        "\n",
        "    if debug:\n",
        "        print('word_cutted_images_list')\n",
        "        for i in range(len(word_cutted_images_list)):\n",
        "            display(Image.fromarray(word_cutted_images_list[i]))\n",
        "\n",
        "        sent_rectangles = transform_bboxes_to_rectangles(sentence_bboxes)\n",
        "        sent_cutted_images_list = create_cutted_images_list(image_array, sent_rectangles)\n",
        "        print('sent_cutted_images_list')\n",
        "        for i in range(len(sent_cutted_images_list)):\n",
        "            display(Image.fromarray(sent_cutted_images_list[i]))\n",
        "\n",
        "\n",
        "    image_data_dictionary = {\n",
        "        \"image_path\": image_path,\n",
        "        \"image_file_name\": image_file_name,\n",
        "        \"image_array\": image_array.copy(),\n",
        "        \"image_width\": image_width,\n",
        "        \"image_height\": image_height, \n",
        "        \"word_bboxes\": word_bboxes,\n",
        "        \"word_rectangles\": word_rectangles,\n",
        "        \"mask_array_from_words\": mask_array_from_words\n",
        "    }\n",
        "    image_data_list = [image_data_dictionary]\n",
        "\n",
        "    w2s_idx = create_word_2_sentence_index(word_bboxes, sentence_bboxes)\n",
        "\n",
        "    w2s_idx_sorted = create_word_2_sentence_index_sorted(w2s_idx, word_rectangles)\n",
        "\n",
        "    opt = create_word_recongnition_args(is_use_second_model=False)\n",
        "    recognition_result_1 = recognition_pipeline(opt, word_cutted_images_list, debug)\n",
        "\n",
        "    if debug:\n",
        "        print('recognition_result_1', recognition_result_1)\n",
        "    \n",
        "    opt = create_word_recongnition_args(is_use_second_model=True, is_sensitive=False)\n",
        "    recognition_result_2 = recognition_pipeline(opt, word_cutted_images_list, debug)\n",
        "    # Нужно для этой модели снизить вероятность у распознования\n",
        "    recognition_result_2 = [(item[0], item[1]*0.7) for item in recognition_result_2]\n",
        "\n",
        "    if debug:\n",
        "        print('recognition_result_2', recognition_result_2)\n",
        "\n",
        "\n",
        "    opt = create_word_recongnition_args(is_use_second_model=True, is_sensitive=True)\n",
        "    recognition_result_sensitive = recognition_pipeline(opt, word_cutted_images_list, debug)\n",
        "\n",
        "    if debug:\n",
        "        print('recognition_result_sensitive', recognition_result_sensitive)\n",
        "\n",
        "    # Из трех моделей распознования слов выбираем с максимальными шансами\n",
        "    final_recognition_result = create_final_recognition_result(recognition_result_1, recognition_result_sensitive)\n",
        "    final_recognition_result = create_final_recognition_result(final_recognition_result, recognition_result_2)\n",
        "\n",
        "    if debug:\n",
        "        print('final_recognition_result', final_recognition_result)\n",
        "\n",
        "    recognized_word_list = [x[0] for x in final_recognition_result]\n",
        "\n",
        "    sentence_dict = create_sentence_dict(w2s_idx_sorted, [x[0] for x in final_recognition_result])\n",
        "\n",
        "    if debug:\n",
        "        print('sentence_dict', sentence_dict)\n",
        "\n",
        "    # # Удаление текста, mode это алгоритм который удаляет, второй это комбинированный из двух алгоритмов\n",
        "    # main(image_data_list, mode=2)\n",
        "\n",
        "    # image_super_resolution_processing(model_isr, image_data_list)\n",
        "\n",
        "    # image_with_deleted_text = Image.fromarray(image_data_list[0][\"image_with_deleted_text\"])\n",
        "\n",
        "    paragraph_bboxes = create_paragraph_bboxes(sentence_bboxes)\n",
        "\n",
        "    sent2para_index = create_word_2_sentence_index(sentence_bboxes, paragraph_bboxes)\n",
        "\n",
        "    sent2para_index_sorted = create_sentence_2_paragraph_index_sorted(sent2para_index, sentence_bboxes)\n",
        "\n",
        "    paragraph_dict = create_paragraph_dict(sent2para_index_sorted, sentence_dict)\n",
        "\n",
        "    if debug:\n",
        "        print('paragraph_dict', paragraph_dict)\n",
        "\n",
        "    #paragraph_dict_translated = translate_paragraph_dict(paragraph_dict, model_translator, tokenizer_translator)\n",
        "\n",
        "    #if debug:\n",
        "    #    print('paragraph_dict_translated', paragraph_dict_translated)\n",
        "\n",
        "    #compiled_image = compile_image(image_with_deleted_text, paragraph_dict_translated, paragraph_bboxes, debug)\n",
        "\n",
        "    #return source_image_for_output, compiled_image\n",
        "    return source_image_for_output, source_image_for_output"
      ],
      "metadata": {
        "id": "DcTeETdCn0OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzeLx7NSaiWZ"
      },
      "source": [
        "## Введите URL до картинки и запустите ячейку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29RkQv1ZQtlC"
      },
      "source": [
        "from google.colab import output\n",
        "input_image_url = 'https://img-9gag-fun.9cache.com/photo/axMNd31_460s.jpg' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "if input_image_url is not None and input_image_url !='':\n",
        "    # source_image, output_image = pipeline(input_image_url, model_isr, model_translator, tokenizer_translator, font, debug=False)\n",
        "    source_image, output_image = pipeline(input_image_url, debug=True)\n",
        "    #output.clear()\n",
        "    #ipyplot.plot_images([source_image, output_image], max_images=2, img_width=output_image.width)\n",
        "else:\n",
        "    print('Provide an image url and try again.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}